{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIRVD_rY243v"
      },
      "source": [
        "### Przykładowe problemy związane ze skalowalnością zadań ML:\n",
        "\n",
        "Ograniczenie CPU: Dane mieszczą się w pamięci RAM, ale proces uczenia trwa za długo. Np. W przypadku konieczności sprawdzenia wielu kombinacji parametrów modelu, wielu modeli, itd.\n",
        "\n",
        "\n",
        "Ograniczenia pamięci: Dane są na tyle duże, że nie mieszczą się w pamięci RAM.\n",
        "\n",
        "\n",
        "#### Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8_ahF6l243x"
      },
      "source": [
        "![](https://github.com/kornisch/ds-notebooks/blob/main/img/ml-Pipeline.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGLY1kC7243x"
      },
      "source": [
        "#### Pipeline Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HElNhGjg243y"
      },
      "source": [
        "![](https://github.com/kornisch/ds-notebooks/blob/main/img/ml-PipelineModel.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVQZI-S4243y"
      },
      "source": [
        "### Potok przetwarzania ML\n",
        "\n",
        "* <b>DataFrame</b>: interfejs API ML używa DataFrame, w którym można przechowywać różne typy danych. Na przykład DataFrame może mieć różne kolumny przechowujące tekst, wektory cech, prawdziwe etykiety i prognozy.\n",
        "\n",
        "* <b>Transformer</b>: Transformator to algorytm, który może przekształcić jedną ramkę danych w inną ramkę danych. Na przykład model ML to transformator, który przekształca ramkę danych z funkcjami w ramkę danych z prognozami. Innym przykładem transformatora jest StringIndexer, który koduje zmienne tekstowe jako zmienne całkowito liczbowe.\n",
        "\n",
        "\n",
        "* <b>Estimator</b>: Estymator to algorytm, który który można zaaplikować do DataFrame w celu wytworzenia transformatora. Np. Algorytm uczenia się jest estymatorem, który trenuje się na DataFrame i tworzy model.\n",
        "\n",
        "\n",
        "* <b>Pipeline</b>: Potok przetwarzania łączy wiele transformatorów i estymatorów razem, aby określić przepływ pracy ML.\n",
        "\n",
        "\n",
        "* <b>Parametr</b>: Wszystkie transformatory i estymatory mają teraz wspólny interfejs API do określania parametrów."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDP-SgOZ243z"
      },
      "source": [
        "### Wczytanie danych"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-f3RaWh243z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "user_name = os.environ.get('USER')\n",
        "print(user_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AB-rr0762430"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".config('spark.driver.memory','1g') \\\n",
        ".config('spark.executor.memory', '2g') \\\n",
        ".getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVIAd7gw2431"
      },
      "outputs": [],
      "source": [
        "gs_path = f'gs://bdg-lab-{user_name}/survey/2020/survey_results_public.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_LIzv_F2431"
      },
      "outputs": [],
      "source": [
        "db_name = user_name.replace('-','_')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0ZTkcB42431"
      },
      "outputs": [],
      "source": [
        "spark.sql(f'DROP DATABASE IF EXISTS {db_name} CASCADE')\n",
        "spark.sql(f'CREATE DATABASE {db_name}')\n",
        "spark.sql(f'USE {db_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kc9AA-mU2432"
      },
      "outputs": [],
      "source": [
        "table_name = \"survey_2020\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6oA_E862432"
      },
      "outputs": [],
      "source": [
        "spark.sql(f'DROP TABLE IF EXISTS {table_name}')\n",
        "\n",
        "spark.sql(f'CREATE TABLE IF NOT EXISTS {table_name} \\\n",
        "          USING csv \\\n",
        "          OPTIONS (HEADER true, INFERSCHEMA true, NULLVALUE \"NA\") \\\n",
        "          LOCATION \"{gs_path}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtIGW4qr2432"
      },
      "outputs": [],
      "source": [
        "spark.sql(f'describe {table_name}').show(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilUIr2Jr2432"
      },
      "source": [
        "### Przygotowanie danych do analizy\n",
        "\n",
        "W ramach zadania chcemy stworzyć klasyfikator, który będzie przewidywać czy respondent zarabia więcej niż 60000 USD rocznie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tI8IPaKO2432"
      },
      "outputs": [],
      "source": [
        "spark_df= spark.sql(f'SELECT *, CAST((convertedComp > 60000) AS STRING) AS compAboveAvg \\\n",
        "                    FROM {table_name} where convertedComp IS NOT NULL ')\n",
        "spark_df.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIvnNGXg2433"
      },
      "source": [
        "<B>Dążymy do tego, żeby przygotować jeden wektor cech oraz jedną kolumnę z oznaczeniami.</B>\n",
        "\n",
        "Kodujemy kolumny tekstowe na numeryczne oraz kodujemy wartości liczbowe na reprezentacje onehotencoder. Następnie dokonujemy asemblacji do jednego wektora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaHg5RRn2433"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "# chcemy przewidziec compAboveAvg\n",
        "y = 'compAboveAvg'\n",
        "# na podstawie:\n",
        "feature_columns = ['OpSys', 'EdLevel', 'MainBranch' , 'Country', 'JobSeek', 'YearsCode']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjgojW2B2433"
      },
      "outputs": [],
      "source": [
        "#Zaczynamy od transformatora StringIndexer, zamieniajacego wartosci 'string' na liczbe\n",
        "# dla cech, ktore zostana wykorzystane do predykcji\n",
        "\n",
        "##### najpierw pokazujemy prosta petle z FOR, a potem zrefactorujmy do list comprehension\n",
        "stringindexer_stages_1 = []\n",
        "for c in feature_columns:\n",
        "    stringindexer_stages_1.append (StringIndexer(inputCol=c, outputCol='stringindexed_' + c).setHandleInvalid(\"keep\"))\n",
        "\n",
        "# i dla zmiennej objaśnianej\n",
        "stringindexer_stages_1.append(StringIndexer(inputCol=y, outputCol='label').setHandleInvalid(\"keep\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku3nOqOc2433"
      },
      "source": [
        "<b>handleInvalid</b> = How to handle invalid data during transform(). Options are 'keep' (invalid data presented as an extra categorical feature) or error (throw an error)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqhqBgSQ2434"
      },
      "outputs": [],
      "source": [
        "# Refactoring do list comprehension\n",
        "stringindexer_stages = [StringIndexer(inputCol=c, outputCol='stringindexed_' + c).setHandleInvalid(\"keep\") for c in feature_columns]\n",
        "\n",
        "# i dla zmiennej objaśnianej\n",
        "stringindexer_stages += [StringIndexer(inputCol=y, outputCol='label').setHandleInvalid(\"keep\")]\n",
        "stringindexer_stages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zwu5vRPf2434"
      },
      "outputs": [],
      "source": [
        "# Po wykonaniu takiej transformacji do DF zostaje dodane  7 nowych kolumn z prefixem \"stringindexed_\"\n",
        "Pipeline(stages=stringindexer_stages).fit(spark_df).transform(spark_df).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRPmlxu12434"
      },
      "outputs": [],
      "source": [
        "onehotencoder_stages = [OneHotEncoder(inputCol='stringindexed_' + c, outputCol='onehot_' + c) for c in feature_columns]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSHHjA5i2434"
      },
      "source": [
        "Rozbudowujemy pipeline:\n",
        "\n",
        "Po wykonaniu takiej transformacji (stringIndexer+onehotencoder) do DF zostaje dodane  6 nowych kolumn z prefixem \"onehot_\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vhx--Sdm2434"
      },
      "outputs": [],
      "source": [
        "\n",
        "pa = Pipeline(stages=stringindexer_stages + onehotencoder_stages).fit(spark_df).transform(spark_df).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvtbCeGd2434"
      },
      "outputs": [],
      "source": [
        "pa.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6KV_TNF2434"
      },
      "source": [
        "Nowe kolumny zawieraja wartosci typu SparseVector zawierajacy mape bitowa.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1MNEOsa2435"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML\n",
        "Image(url= \"https://miro.medium.com/max/2400/1*ggtP4a5YaRx6l09KQaYOnw.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0eODsrg2435"
      },
      "outputs": [],
      "source": [
        "print(\"Orginal values:\")\n",
        "print(pa['OpSys'].unique())\n",
        "print (\"---------\")\n",
        "print(\"StringIndexed values:\")\n",
        "print(pa['stringindexed_OpSys'].unique())\n",
        "print (\"---------\")\n",
        "print(\"OneHot values:\")\n",
        "print(pa['onehot_OpSys'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHUQ0CpV2435"
      },
      "source": [
        "#### <B>Asemblacja</B> - połączenie wszystkich kolumn predykcyjnych do jednej (kolumna features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBViDl822435"
      },
      "outputs": [],
      "source": [
        "extracted_columns = ['onehot_' + c for c in feature_columns]\n",
        "vectorassembler_stage = VectorAssembler(inputCols=extracted_columns, outputCol='features')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceT4sdCe2435"
      },
      "source": [
        "### Połączenie wszystkich krokþw przygotowania danych w jednym potoku przetwarzania (pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZbB-hNB2435"
      },
      "outputs": [],
      "source": [
        "# wybór kolumn do ostatecznej ramki danych\n",
        "# poza kolumnami features i label (które będą wykorzystywane do budowania modelu)\n",
        "# zostawiamy m.in. oryginalne kolumn (feature_columns)\n",
        "final_columns = [y] + feature_columns + extracted_columns + ['features', 'label']\n",
        "final_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tTNJIzi2436"
      },
      "outputs": [],
      "source": [
        "transformed_df = Pipeline(stages=stringindexer_stages + \\\n",
        "                          onehotencoder_stages + \\\n",
        "                          [vectorassembler_stage]).fit(spark_df).transform(spark_df).select(final_columns)\n",
        "\n",
        "transformed_df.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiHs6myO2436"
      },
      "source": [
        "### Podzial na zbior treningowy/testowy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gwyx12ao2436"
      },
      "outputs": [],
      "source": [
        "training, test = transformed_df.randomSplit([0.8, 0.2], seed=1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x86TCof02436"
      },
      "outputs": [],
      "source": [
        "training.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_yuMY_C2436"
      },
      "source": [
        "### Uczenie modelu - model.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rk3_RlfL2437"
      },
      "outputs": [],
      "source": [
        "# na poczatek wybierzemy drzewo decyzyjne. Nie musimy podawac zadnych parametrow\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier(featuresCol='features', labelCol='label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l122qhdR2437"
      },
      "outputs": [],
      "source": [
        "simple_model = Pipeline(stages=[dt]).fit(training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5K7DUFS2437"
      },
      "outputs": [],
      "source": [
        "simple_model.stages[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5XJcSam2437"
      },
      "source": [
        "### Predykcja - model.transform()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhmaOGeF2438"
      },
      "outputs": [],
      "source": [
        "pred_simple = simple_model.transform(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUWqcdUu2438"
      },
      "outputs": [],
      "source": [
        "show_columns = final_columns + ['prediction', 'rawPrediction', 'probability']\n",
        "pred_simple.limit(5).select(show_columns).toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlKkTcNX2438"
      },
      "source": [
        "## Ewaluacje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUkuKThH2438"
      },
      "outputs": [],
      "source": [
        "# macierz pomyłek (confusion matrix)\n",
        "label_and_pred = pred_simple.select('label', 'prediction')\n",
        "label_and_pred.groupBy('label', 'prediction').count().toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opK9q5r82438"
      },
      "outputs": [],
      "source": [
        "# Ewaluator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqOpbZWu2438"
      },
      "outputs": [],
      "source": [
        "auroc_simple = evaluator.evaluate(pred_simple)\n",
        "auroc_simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLg2cCiS2439"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator_m = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator_m.evaluate(pred_simple)\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5b1w1212439"
      },
      "source": [
        "## Dodanie hiperparametrów"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbgNtIBW2439"
      },
      "outputs": [],
      "source": [
        "# Jakie wartości hiperparametru maxDepth mają być przetestowane:\n",
        "from pyspark.ml.tuning import ParamGridBuilder\n",
        "param_grid = ParamGridBuilder().\\\n",
        "    addGrid(dt.maxDepth, [2,3,4,5,6]).\\\n",
        "    build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbLf6Lcp2439"
      },
      "outputs": [],
      "source": [
        "# Walidacja krzyżowa wykonwyana w celu optymalizacji hiperparametrów\n",
        "from pyspark.ml.tuning import CrossValidator\n",
        "cv = CrossValidator(estimator=dt, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_w6Zrn22439"
      },
      "outputs": [],
      "source": [
        "# Budowa modelu na podstawie danych treningowych\n",
        "cv_model = cv.fit(training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa_igMYP243-"
      },
      "outputs": [],
      "source": [
        "cv_model.bestModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF4xhs_p243-"
      },
      "source": [
        "## Predykcja z nowym modelem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqVzDG2z243-"
      },
      "outputs": [],
      "source": [
        "# Jak wygląda predykcja na zbiorze danych testowych?\n",
        "pred_cv = cv_model.transform(test)\n",
        "show_columns = final_columns + ['prediction', 'rawPrediction', 'probability']\n",
        "pred_cv.limit(5).select(show_columns).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZcUV46_243-"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "label_and_pred = pred_cv.select('label', 'prediction')\n",
        "label_and_pred.groupBy('label', 'prediction').count().toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIIV9omf243-"
      },
      "outputs": [],
      "source": [
        "auroc_cv = evaluator.evaluate(pred_cv)\n",
        "auroc_cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9rXLk3P243_"
      },
      "outputs": [],
      "source": [
        "acc_cv = evaluator_m.evaluate(pred_cv)\n",
        "acc_cv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBvdQ3UO243_"
      },
      "source": [
        "## Klasyfikacja za pomoca Gradient Boosted Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atDOCIs4243_"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
        "model = gbt.fit(training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4Lkpr7Q243_"
      },
      "outputs": [],
      "source": [
        "evaluator.evaluate(model.transform(test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEETid9I243_"
      },
      "source": [
        "## Zadania:\n",
        "\n",
        "* Czy mozna jeszcze poprawic jakosc predykcji:\n",
        "    * a) dodajac cechy\n",
        "    * b) zmieniajac model\n",
        "    * c) lepiej dobierajac parametry modelu ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RidzQcdB243_"
      },
      "outputs": [],
      "source": [
        "#Kod w R\n",
        "#library(data.table)\n",
        "#srv <- fread(\"survey_results_public.csv\")\n",
        "#srv$OpSys2 <- srv$OpSys == \"Windows\"\n",
        "#library(rpart)\n",
        "#srv$CompAboveAvg <- CompAboveAvg$ConvertedComp > 60e3\n",
        "#dt_fit = rpart(CompAboveAvg ~ Age + EdLevel + JobSeek + OpSys + YearsCode , data = srv, method = 'class')\n",
        "#pred_y = predict(dt_fit, type = 'class')\n",
        "#table(predict(dt_fit, srv[,c(\"Age\" , \"EdLevel\", \"JobSeek\", \"OpSys\", \"YearsCode\")], type = \"class\"), srv$CompAboveAvg)\n",
        "#srv(cor)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "datascience",
      "language": "python",
      "name": "datascience"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "notebook_test": {
      "keytab_path": "/data/work/home/ds-lab-testuser1/ds-lab-testuser1.keytab",
      "user": "ds-lab-testuser1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}